{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import tree\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "import gensim\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17154274, 22638920)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "Y = df.iloc[::,1].to_numpy()\n",
    "types = {'age':0,\n",
    "         'ethnicity':1,\n",
    "         'gender':2,\n",
    "         'not_cyberbullying':3,\n",
    "         'other_cyberbullying':4,\n",
    "         'religion':5}\n",
    "Y = [types[y] for y in Y]\n",
    "Y = np.reshape(Y, (len(Y),1))\n",
    "X = df.iloc[::,0].to_numpy()\n",
    "X = [''.join(item.lower() for item in x if item.isalpha() or item == \" \") for x in X]\n",
    "X = [x.split(\" \") for x in X] \n",
    "#X = [item for sublist in X for item in sublist]\n",
    "\n",
    "model_w2v = gensim.models.Word2Vec(\n",
    "            X,\n",
    "            vector_size=50, # desired no. of features/independent variables\n",
    "            window=5, # context window size\n",
    "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
    "            sg = 1, # 1 for skip-gram model\n",
    "            hs = 0,\n",
    "            negative = 10, # for negative sampling\n",
    "            workers= 32, # no.of cores\n",
    "            seed = 34) \n",
    "\n",
    "model_w2v.train(X, total_examples= len(X), epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/paulloubet/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "### PARAMETERS ###\n",
    "# if stemming, else lemmatizing words\n",
    "use_stemm = True\n",
    "# dimension vectors needed\n",
    "dimension_selected = 100\n",
    "\n",
    "##################\n",
    "# stop words\n",
    "list_stp_wrd = stopwords.words('english')\n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Init stemmer\n",
    "stemmer = PorterStemmer()\n",
    "# set were we get the new dataset with lemmatize words\n",
    "lemmatize_X = []\n",
    "\n",
    "#for each sentence (list of word)\n",
    "for sentence_wrd in X:\n",
    "    sentence_wrd = [x  for x in sentence_wrd if x not in list_stp_wrd]\n",
    "    # we lemmatize each wrd\n",
    "    if use_stemm:\n",
    "        # stemming if necessary\n",
    "        lemm_sentence_wrd = [stemmer.stem(wrd) for wrd in sentence_wrd]\n",
    "    else:\n",
    "        # else lemmatize\n",
    "        lemm_sentence_wrd = [lemmatizer.lemmatize(wrd) for wrd in sentence_wrd]\n",
    "        \n",
    "    lemmatize_X.append(lemm_sentence_wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a dictionnary\n",
    "import itertools\n",
    "all_words = list(itertools.chain.from_iterable(lemmatize_X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# remove empty caracters\n",
    "all_words = [x for x in all_words if x != '']\n",
    "# count occurence of each word\n",
    "dist_counter = Counter(all_words)\n",
    "\n",
    "# sorted by the most frequents words\n",
    "sorted_dict = sorted(dist_counter.items(),key=lambda x: x[1] , reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters of words wee keep\n",
    "\n",
    "\n",
    "sorted_dict = sorted_dict[:dimension_selected]\n",
    "\n",
    "# get the list of most frequent words\n",
    "mf_words = [x[0] for x in sorted_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BOW = []\n",
    "for sentence_word in lemmatize_X:\n",
    "    vector = []\n",
    "    for word in mf_words :\n",
    "        # add the number of time the word is in the sentence\n",
    "        vector.append(sentence_word.count(word))\n",
    "    # add the vector construct\n",
    "    BOW.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data = pd.DataFrame(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwordvec_df = pd.DataFrame(wordvec_arrays)\\nwordvec_df.shape\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_vector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += model_w2v.wv[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:  # handling the case where the token is not in vocabulary\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "wordvec_arrays = np.zeros((len(X), 50)) \n",
    "for i in range(len(X)):\n",
    "    wordvec_arrays[i,:] = word_vector(X[i], 50)\n",
    "\n",
    "\"\"\"\n",
    "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
    "wordvec_df.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree():\n",
    "    def __init__(self, max_depth, min_samples_split=2, min_samples_leaf=1, splitting=10):\n",
    "        \"\"\"Tree Descision Classifier.\n",
    "\n",
    "        Args:\n",
    "            max_depth (int): The maximum depth of the tree.\n",
    "            \n",
    "            min_samples_split (int): The minimum number of samples\n",
    "                required to split an internal node.\n",
    "                \n",
    "            min_samples_leaf (int): The minimum number of samples\n",
    "                required to be at a leaf node\n",
    "        \"\"\"\n",
    "        assert max_depth >= 1, \"max_depth must be greater or equal than 1\"\n",
    "        assert min_samples_split >= 2, \"min_samples_split must be greater or equal than 2\"\n",
    "        assert min_samples_leaf >= 1, \"min_samples_leaf must be greater or equal than 1\"\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.splitting = splitting\n",
    "        self.nodes = {\"root\": {}}\n",
    "        \n",
    "    def gini_index(self, sub, m):\n",
    "        n = len(sub)\n",
    "        proportions = sum([((sub[:,-1] == x).sum()/n)**2 for x in np.unique(sub[:,-1])])\n",
    "        return (1-proportions) * (n/m)\n",
    "        \n",
    "        \n",
    "    def get_split(self, X, depth, node):\n",
    "        \n",
    "        m = len(X)\n",
    "        \n",
    "        if depth != 0 and m >= self.min_samples_split:\n",
    "            \n",
    "            best_split = None\n",
    "            best_feature = None\n",
    "            best_value = float(\"inf\")\n",
    "            \n",
    "            for feature in range(len(X[0]) - 1):\n",
    "                if self.splitting is None:\n",
    "                    uniques = np.unique(X[:,feature])\n",
    "                else:\n",
    "                    all_sorted = sorted(X[:,feature])\n",
    "                    batch = len(all_sorted) // self.splitting\n",
    "                    uniques = [all_sorted[i*batch] for i in range(self.splitting)]\n",
    "                for split in uniques:\n",
    "                    A, B = X[X[:,feature] <= split], X[X[:,feature] > split]\n",
    "                    if len(A) >= self.min_samples_leaf and len(B) >= self.min_samples_leaf:\n",
    "                        value = self.gini_index(A, m) + self.gini_index(B, m)\n",
    "                        if value < best_value:\n",
    "                            best_value = value\n",
    "                            best_feature = feature\n",
    "                            best_split = split\n",
    "            \n",
    "            if best_feature is not None:\n",
    "                A, B = X[X[:,best_feature] <= best_split], X[X[:,best_feature] > best_split]\n",
    "                node[\"feature\"] = best_feature\n",
    "                node[\"split\"] = best_split\n",
    "                node[\"A\"] = {}\n",
    "                node[\"B\"] = {}\n",
    "                node[\"class_A\"] = np.unique(A[:,-1])[np.argmax([(A[:,-1] == x).sum() for x in np.unique(A[:,-1])])]\n",
    "                node[\"class_B\"] = np.unique(B[:,-1])[np.argmax([(B[:,-1] == x).sum() for x in np.unique(B[:,-1])])]\n",
    "                self.get_split(A, depth-1, node[\"A\"])\n",
    "                self.get_split(B, depth-1, node[\"B\"])\n",
    "                \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        X = np.append(X,y, axis=1)\n",
    "        self.get_split(X, self.max_depth, self.nodes[\"root\"])\n",
    "        \n",
    "    def predict(self, X):\n",
    "        node = self.nodes[\"root\"]\n",
    "        while True:\n",
    "            if X[node[\"feature\"]] <= node[\"split\"]:\n",
    "                if not node[\"A\"]:\n",
    "                    return node[\"class_A\"]\n",
    "                else:\n",
    "                    node = node[\"A\"]\n",
    "            else:\n",
    "                if not node[\"B\"]:\n",
    "                    return node[\"class_B\"]\n",
    "                else:\n",
    "                    node = node[\"B\"]\n",
    "    \n",
    "    def score(self, X, Y):\n",
    "        count = 0\n",
    "        for x, y in zip(X,Y):\n",
    "            if self.predict(x) == y: count += 1\n",
    "        return count/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-learn model score:  0.7042913055633212\n",
      "Homemade model score:  0.7045708694436679\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_data, Y, test_size=0.30, random_state=0)\n",
    "X_train = X_train.values.tolist()\n",
    "X_test = X_test.values.tolist()\n",
    "\n",
    "clf = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 6, min_samples_leaf= 1, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"sk-learn model score: \", clf.score(X_test, y_test))\n",
    "\n",
    "model = Tree(max_depth=10, min_samples_split = 6, min_samples_leaf= 1, splitting=10)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Homemade model score: \", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__(self, n_estimators):\n",
    "        self.n_estimators = n_estimators\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.all_trees = []\n",
    "        sub_size = round(len(X)*(2/3))\n",
    "        for i in range(self.n_estimators):\n",
    "            id = np.random.randint(0,len(X),sub_size)\n",
    "            subX = np.array(X)[id]\n",
    "            suby = y[id.astype(int)]\n",
    "            t = Tree(10)\n",
    "            t.fit(subX, suby)\n",
    "            self.all_trees.append(t)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for t in self.all_trees:\n",
    "            predictions.append(t.predict(X))\n",
    "        return max(set(predictions), key=predictions.count)\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        count = 0\n",
    "        for x, y in zip(X,Y):\n",
    "            if self.predict(x) == y: count += 1\n",
    "        return count/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-af4644b4337c>:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(X_train, y_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-learn model score:  0.788160469667319\n",
      "Homemade model score:  0.7054095610847079\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_data, Y, test_size=0.30, random_state=0)\n",
    "X_train = X_train.values.tolist()\n",
    "X_test = X_test.values.tolist()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"sk-learn model score: \", clf.score(X_test, y_test))\n",
    "\n",
    "model = RandomForest(10)\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Homemade model score: \", model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07429fcb4c504a0378980367d3c7fce80391802e4a36d1633a5a7bb6cd53a327"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
