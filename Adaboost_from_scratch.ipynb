{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "pFqzLRMgCmIE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn import tree\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
        "import gensim\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d_rQct77CmIG"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
        "Y = df.iloc[::,1].to_numpy()\n",
        "types = {'age':0,\n",
        "         'ethnicity':1,\n",
        "         'gender':2,\n",
        "         'not_cyberbullying':3,\n",
        "         'other_cyberbullying':4,\n",
        "         'religion':5}\n",
        "Y = [types[y] for y in Y]\n",
        "Y = np.reshape(Y, (len(Y),1))\n",
        "X = df.iloc[::,0].to_numpy()\n",
        "X = [''.join(item.lower() for item in x if item.isalpha() or item == \" \") for x in X]\n",
        "X = [x.split(\" \") for x in X] \n",
        "#X = [item for sublist in X for item in sublist]\n",
        "\n",
        "model_w2v = gensim.models.Word2Vec(\n",
        "            X,\n",
        "            vector_size=50, # desired no. of features/independent variables\n",
        "            window=5, # context window size\n",
        "            min_count=2, # Ignores all words with total frequency lower than 2.                                  \n",
        "            sg = 1, # 1 for skip-gram model\n",
        "            hs = 0,\n",
        "            negative = 10, # for negative sampling\n",
        "            workers= 32, # no.of cores\n",
        "            seed = 34) \n",
        "\n",
        "#model_w2v.train(X, total_examples= len(X), epochs=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk9q-RPkCmIH",
        "outputId": "68ddf2ed-a71e-4d8a-9014-2f75191c42c7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(47692, 50)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def word_vector(tokens, size):\n",
        "    vec = np.zeros(size).reshape((1, size))\n",
        "    count = 0\n",
        "    for word in tokens:\n",
        "        try:\n",
        "            vec += model_w2v.wv[word].reshape((1, size))\n",
        "            count += 1.\n",
        "        except KeyError:  # handling the case where the token is not in vocabulary\n",
        "            continue\n",
        "    if count != 0:\n",
        "        vec /= count\n",
        "    return vec\n",
        "wordvec_arrays = np.zeros((len(X), 50)) \n",
        "for i in range(len(X)):\n",
        "    wordvec_arrays[i,:] = word_vector(X[i], 50)\n",
        "wordvec_df = pd.DataFrame(wordvec_arrays)\n",
        "wordvec_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2rXVHJpoCmIJ"
      },
      "outputs": [],
      "source": [
        "class Tree():\n",
        "    def __init__(self, max_depth, min_samples_split=2, min_samples_leaf=1, splitting=10):\n",
        "        \"\"\"Tree Descision Classifier.\n",
        "\n",
        "        Args:\n",
        "            max_depth (int): The maximum depth of the tree.\n",
        "            \n",
        "            min_samples_split (int): The minimum number of samples\n",
        "                required to split an internal node.\n",
        "                \n",
        "            min_samples_leaf (int): The minimum number of samples\n",
        "                required to be at a leaf node\n",
        "        \"\"\"\n",
        "        assert max_depth >= 1, \"max_depth must be greater or equal than 1\"\n",
        "        assert min_samples_split >= 2, \"min_samples_split must be greater or equal than 2\"\n",
        "        assert min_samples_leaf >= 1, \"min_samples_leaf must be greater or equal than 1\"\n",
        "        \n",
        "        self.max_depth = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "        self.min_samples_leaf = min_samples_leaf\n",
        "        self.splitting = splitting\n",
        "        self.nodes = {\"root\": {}}\n",
        "        \n",
        "    def gini_index(self, sub, m):\n",
        "        n = len(sub)\n",
        "        proportions = sum([((sub[:,-1] == x).sum()/n)**2 for x in np.unique(sub[:,-1])])\n",
        "        return (1-proportions) * (n/m)\n",
        "        \n",
        "        \n",
        "    def get_split(self, X, depth, node):\n",
        "        \n",
        "        m = len(X)\n",
        "        \n",
        "        if depth != 0 and m >= self.min_samples_split:\n",
        "            \n",
        "            best_split = None\n",
        "            best_feature = None\n",
        "            best_value = float(\"inf\")\n",
        "            \n",
        "            for feature in range(len(X[0]) - 1):\n",
        "                if self.splitting is None:\n",
        "                    uniques = np.unique(X[:,feature])\n",
        "                else:\n",
        "                    all_sorted = sorted(X[:,feature])\n",
        "                    batch = len(all_sorted) // self.splitting\n",
        "                    uniques = [all_sorted[i*batch] for i in range(self.splitting)]\n",
        "                for split in uniques:\n",
        "                    A, B = X[X[:,feature] <= split], X[X[:,feature] > split]\n",
        "                    if len(A) >= self.min_samples_leaf and len(B) >= self.min_samples_leaf:\n",
        "                        value = self.gini_index(A, m) + self.gini_index(B, m)\n",
        "                        if value < best_value:\n",
        "                            best_value = value\n",
        "                            best_feature = feature\n",
        "                            best_split = split\n",
        "            \n",
        "            if best_feature is not None:\n",
        "                A, B = X[X[:,best_feature] <= best_split], X[X[:,best_feature] > best_split]\n",
        "                node[\"feature\"] = best_feature\n",
        "                node[\"split\"] = best_split\n",
        "                node[\"A\"] = {}\n",
        "                node[\"B\"] = {}\n",
        "                node[\"class_A\"] = np.unique(A[:,-1])[np.argmax([(A[:,-1] == x).sum() for x in np.unique(A[:,-1])])]\n",
        "                node[\"class_B\"] = np.unique(B[:,-1])[np.argmax([(B[:,-1] == x).sum() for x in np.unique(B[:,-1])])]\n",
        "                self.get_split(A, depth-1, node[\"A\"])\n",
        "                self.get_split(B, depth-1, node[\"B\"])\n",
        "                \n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        X = np.append(X,y, axis=1)\n",
        "        self.get_split(X, self.max_depth, self.nodes[\"root\"])\n",
        "        \n",
        "    def predict(self, X):\n",
        "        node = self.nodes[\"root\"]\n",
        "        while True:\n",
        "            if X[node[\"feature\"]] <= node[\"split\"]:\n",
        "                if not node[\"A\"]:\n",
        "                    return node[\"class_A\"]\n",
        "                else:\n",
        "                    node = node[\"A\"]\n",
        "            else:\n",
        "                if not node[\"B\"]:\n",
        "                    return node[\"class_B\"]\n",
        "                else:\n",
        "                    node = node[\"B\"]\n",
        "        print(\"nothing returned\")\n",
        "    \n",
        "    def score(self, X, Y):\n",
        "        count = 0\n",
        "        for x, y in zip(X,Y):\n",
        "            if self.predict(x) == y: count += 1\n",
        "        return count/len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "DJ010WQxCmIL",
        "outputId": "ba9a3823-b6a1-4fc2-d48a-868e5e810fc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sk-learn model score:  0.6668297455968689\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nmodel = Tree(max_depth=10, min_samples_split = 6, min_samples_leaf= 1, splitting=10)\\nmodel.fit(X_train, y_train)\\nprint(\"Homemade model score: \", model.score(X_test, y_test))'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(wordvec_df, Y, test_size=0.30, random_state=0)\n",
        "X_train = X_train.values.tolist()\n",
        "X_test = X_test.values.tolist()\n",
        "\n",
        "clf = tree.DecisionTreeClassifier(max_depth = 10, min_samples_split = 6, min_samples_leaf= 1, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"sk-learn model score: \", clf.score(X_test, y_test))\n",
        "\"\"\"\n",
        "model = Tree(max_depth=10, min_samples_split = 6, min_samples_leaf= 1, splitting=10)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Homemade model score: \", model.score(X_test, y_test))\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "_4PxJzTqCmIM"
      },
      "outputs": [],
      "source": [
        "class RandomForest():\n",
        "    def __init__(self, n_estimators):\n",
        "        self.n_estimators = n_estimators\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        self.all_trees = []\n",
        "        sub_size = round(len(X)*(2/3))\n",
        "        for i in range(self.n_estimators):\n",
        "            id = np.random.randint(0,len(X),sub_size)\n",
        "            subX = np.array(X)[id]\n",
        "            suby = y[id.astype(int)]\n",
        "            t = Tree(10)\n",
        "            t.fit(subX, suby)\n",
        "            self.all_trees.append(t)\n",
        "            \n",
        "    def predict(self, X):\n",
        "        predictions = []\n",
        "        for t in self.all_trees:\n",
        "            predictions.append(t.predict(X))\n",
        "        return max(set(predictions), key=predictions.count)\n",
        "\n",
        "    def score(self, X, Y):\n",
        "        count = 0\n",
        "        for x, y in zip(X,Y):\n",
        "            if self.predict(x) == y: count += 1\n",
        "        return count/len(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "PKHywUIdCmIM",
        "outputId": "43dc56ea-d67c-4505-96e5-e8e7f430f8e5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/z6/lk5zb8l547v8zf8nv88bj08w0000gn/T/ipykernel_20767/2407923744.py:6: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
            "  clf.fit(X_train, y_train)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sk-learn model score:  0.7128180039138943\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nmodel = RandomForest(10)\\nmodel.fit(X_train, y_train)\\nprint(\"Homemade model score: \", model.score(X_test, y_test))'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(wordvec_df, Y, test_size=0.30, random_state=0)\n",
        "X_train = X_train.values.tolist()\n",
        "X_test = X_test.values.tolist()\n",
        "\n",
        "clf = RandomForestClassifier(n_estimators=10)\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"sk-learn model score: \", clf.score(X_test, y_test))\n",
        "\"\"\"\n",
        "model = RandomForest(10)\n",
        "model.fit(X_train, y_train)\n",
        "print(\"Homemade model score: \", model.score(X_test, y_test))\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0sPw2OECmIN"
      },
      "source": [
        "# AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NlCvZFtpCmIO"
      },
      "outputs": [],
      "source": [
        "# Compute error rate, alpha and w\n",
        "def compute_error(y, y_pred, w_i):\n",
        "    '''\n",
        "    Calculate the error rate of a weak classifier m. Arguments:\n",
        "    y: actual target value\n",
        "    y_pred: predicted value by weak classifier\n",
        "    w_i: individual weights for each observation\n",
        "    \n",
        "    Note that all arrays should be the same length\n",
        "    '''\n",
        "    return (sum(w_i * (np.not_equal(y, y_pred)).astype(int)))/sum(w_i)\n",
        "\n",
        "def compute_alpha(error):\n",
        "    '''\n",
        "    Calculate the weight of a weak classifier m in the majority vote of the final classifier. This is called\n",
        "    alpha in chapter 10.1 of The Elements of Statistical Learning. Arguments:\n",
        "    error: error rate from weak classifier m\n",
        "    '''\n",
        "    return np.log((1 - error) / error)\n",
        "\n",
        "def update_weights(w_i, alpha, y, y_pred):\n",
        "    ''' \n",
        "    Update individual weights w_i after a boosting iteration. Arguments:\n",
        "    w_i: individual weights for each observation\n",
        "    y: actual target value\n",
        "    y_pred: predicted value by weak classifier  \n",
        "    alpha: weight of weak classifier used to estimate y_pred\n",
        "    '''  \n",
        "    return w_i * np.exp(alpha * (np.not_equal(y, y_pred)).astype(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yir4zoiARIek"
      },
      "outputs": [],
      "source": [
        "def adapt_format(el, alpha):\n",
        "    formated = [0]*6\n",
        "    formated[el]= alpha \n",
        "    return formated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dGK-ho3JCmIO"
      },
      "outputs": [],
      "source": [
        "class AdaBoost:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.alphas = []\n",
        "        self.G_M = []\n",
        "        self.M = None\n",
        "        self.training_errors = []\n",
        "        self.prediction_errors = []\n",
        "\n",
        "    def fit(self, X, y, M = 100):\n",
        "        '''\n",
        "        Fit model. Arguments:\n",
        "        X: independent variables - array-like matrix\n",
        "        y: target variable - array-like vector\n",
        "        M: number of boosting rounds. Default is 100 - integer\n",
        "        '''\n",
        "        \n",
        "        # reset parameters\n",
        "        self.alphas = [] \n",
        "        self.training_errors = []\n",
        "        self.M = M\n",
        "\n",
        "        # fittinf each new classifier\n",
        "        for m in range(0, M):\n",
        "            \n",
        "            if m == 0:\n",
        "                w_i = np.ones(len(y)) * 1 / len(y)  \n",
        "            else:\n",
        "                #updateweight \n",
        "                w_i = update_weights(w_i, alpha_m, y, y_pred)\n",
        "            \n",
        "            # feat new classifier\n",
        "            G_m = tree.DecisionTreeClassifier(max_depth = 10)     \n",
        "            G_m.fit(X, y, sample_weight = w_i)\n",
        "            y_pred = G_m.predict(X)\n",
        "            \n",
        "            \n",
        "            self.G_M.append(G_m) # save classifier\n",
        "\n",
        "            # Get error\n",
        "            error_m = compute_error(y, y_pred, w_i)\n",
        "            self.training_errors.append(error_m)\n",
        "            #  Compute alpha based on the error\n",
        "            alpha_m = compute_alpha(error_m)\n",
        "            self.alphas.append(alpha_m)\n",
        "\n",
        "        assert len(self.G_M) == len(self.alphas)\n",
        "\n",
        "    def predict(self, X):\n",
        "      '''\n",
        "      Predict using fitted model. Arguments:\n",
        "      X: independent variables - array-like\n",
        "      '''\n",
        "\n",
        "      # initialise array to save results\n",
        "      weak_preds = np.array([[0.0]*6]*len(X))\n",
        "\n",
        "      for m in range(self.M):\n",
        "          y_pred_m = self.G_M[m].predict(X)\n",
        "          reformated = [adapt_format(x, self.alphas[m] )for x in y_pred_m]\n",
        "\n",
        "          for i in range(len(reformated)):\n",
        "              weak_preds[i]+= np.array(reformated[i])\n",
        "\n",
        "      # Calculate final predictions by getting the max \n",
        "      y_pred = [x.argmax() for x in weak_preds]\n",
        "\n",
        "      return y_pred\n",
        "\n",
        "    def score(self, X, Y):\n",
        "      count = 0\n",
        "      preds = self.predict(X)\n",
        "      for i in range(len(Y)):\n",
        "          if preds[i] == Y[i]: count += 1\n",
        "      return count/len(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BZj6XFvnCmIP"
      },
      "outputs": [],
      "source": [
        "#len(X_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPF_iZ0UUifs",
        "outputId": "58f74fb9-03df-47f0-ce4e-13a5170ab514"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.array([2,0,0,0,1]).argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 249
        },
        "id": "vpVDNtsVCmIQ",
        "outputId": "e5b38aef-9d3b-402c-8336-a2a923fcf39e"
      },
      "outputs": [],
      "source": [
        "y_train = [x[0] for x in y_train]\n",
        "y_test = [x[0] for x in y_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rXsyR1zeCmIQ"
      },
      "outputs": [],
      "source": [
        "# Fit model\n",
        "ab = AdaBoost()\n",
        "ab.fit(X_train, y_train, M = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_AZpzJw_CmIR"
      },
      "outputs": [],
      "source": [
        "y_pred = ab.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z43weQiqCmIR",
        "outputId": "b8e25395-4714-494a-c795-88dd9fc67642"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7134470226446743"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ab.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-2GX5ARYVlD"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "cyberbully_with_boosting.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "07429fcb4c504a0378980367d3c7fce80391802e4a36d1633a5a7bb6cd53a327"
    },
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
